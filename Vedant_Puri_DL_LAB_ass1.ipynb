{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUrKqJu8qrCz",
        "outputId": "44fb62a0-7f3c-469e-da86-34e354468b70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.2513673360948589\n",
            "Epoch 100, Loss: 0.2500124624027848\n",
            "Epoch 200, Loss: 0.2500057123792562\n",
            "Epoch 300, Loss: 0.2500047207608552\n",
            "Epoch 400, Loss: 0.25000376701954014\n",
            "Epoch 500, Loss: 0.2500028262810234\n",
            "Epoch 600, Loss: 0.25000189770575987\n",
            "Epoch 700, Loss: 0.250000980585594\n",
            "Epoch 800, Loss: 0.25000007424178644\n",
            "Epoch 900, Loss: 0.24999917802267643\n",
            "Epoch 1000, Loss: 0.2499982913018494\n",
            "Epoch 1100, Loss: 0.24999741347642732\n",
            "Epoch 1200, Loss: 0.24999654396547308\n",
            "Epoch 1300, Loss: 0.2499956822084994\n",
            "Epoch 1400, Loss: 0.24999482766407513\n",
            "Epoch 1500, Loss: 0.24999397980851862\n",
            "Epoch 1600, Loss: 0.24999313813467428\n",
            "Epoch 1700, Loss: 0.2499923021507627\n",
            "Epoch 1800, Loss: 0.249991471379301\n",
            "Epoch 1900, Loss: 0.24999064535608617\n",
            "Epoch 2000, Loss: 0.24998982362923672\n",
            "Epoch 2100, Loss: 0.2499890057582893\n",
            "Epoch 2200, Loss: 0.24998819131334393\n",
            "Epoch 2300, Loss: 0.24998737987425562\n",
            "Epoch 2400, Loss: 0.24998657102986796\n",
            "Epoch 2500, Loss: 0.24998576437728554\n",
            "Epoch 2600, Loss: 0.24998495952118202\n",
            "Epoch 2700, Loss: 0.249984156073141\n",
            "Epoch 2800, Loss: 0.24998335365102758\n",
            "Epoch 2900, Loss: 0.24998255187838647\n",
            "Epoch 3000, Loss: 0.24998175038386677\n",
            "Epoch 3100, Loss: 0.24998094880066934\n",
            "Epoch 3200, Loss: 0.24998014676601546\n",
            "Epoch 3300, Loss: 0.24997934392063492\n",
            "Epoch 3400, Loss: 0.24997853990827182\n",
            "Epoch 3500, Loss: 0.24997773437520632\n",
            "Epoch 3600, Loss: 0.2499769269697908\n",
            "Epoch 3700, Loss: 0.24997611734199884\n",
            "Epoch 3800, Loss: 0.24997530514298627\n",
            "Epoch 3900, Loss: 0.24997449002466163\n",
            "Epoch 4000, Loss: 0.24997367163926681\n",
            "Epoch 4100, Loss: 0.24997284963896457\n",
            "Epoch 4200, Loss: 0.2499720236754333\n",
            "Epoch 4300, Loss: 0.2499711933994673\n",
            "Epoch 4400, Loss: 0.2499703584605816\n",
            "Epoch 4500, Loss: 0.24996951850662047\n",
            "Epoch 4600, Loss: 0.24996867318336874\n",
            "Epoch 4700, Loss: 0.24996782213416424\n",
            "Epoch 4800, Loss: 0.24996696499951176\n",
            "Epoch 4900, Loss: 0.24996610141669648\n",
            "Epoch 5000, Loss: 0.24996523101939605\n",
            "Epoch 5100, Loss: 0.24996435343729168\n",
            "Epoch 5200, Loss: 0.2499634682956759\n",
            "Epoch 5300, Loss: 0.2499625752150567\n",
            "Epoch 5400, Loss: 0.2499616738107574\n",
            "Epoch 5500, Loss: 0.2499607636925114\n",
            "Epoch 5600, Loss: 0.24995984446405078\n",
            "Epoch 5700, Loss: 0.2499589157226873\n",
            "Epoch 5800, Loss: 0.2499579770588864\n",
            "Epoch 5900, Loss: 0.249957028055832\n",
            "Epoch 6000, Loss: 0.249956068288982\n",
            "Epoch 6100, Loss: 0.2499550973256125\n",
            "Epoch 6200, Loss: 0.24995411472435144\n",
            "Epoch 6300, Loss: 0.2499531200346995\n",
            "Epoch 6400, Loss: 0.24995211279653734\n",
            "Epoch 6500, Loss: 0.24995109253961847\n",
            "Epoch 6600, Loss: 0.2499500587830473\n",
            "Epoch 6700, Loss: 0.24994901103473946\n",
            "Epoch 6800, Loss: 0.2499479487908654\n",
            "Epoch 6900, Loss: 0.24994687153527487\n",
            "Epoch 7000, Loss: 0.24994577873890117\n",
            "Epoch 7100, Loss: 0.24994466985914368\n",
            "Epoch 7200, Loss: 0.24994354433922908\n",
            "Epoch 7300, Loss: 0.24994240160754636\n",
            "Epoch 7400, Loss: 0.24994124107695853\n",
            "Epoch 7500, Loss: 0.24994006214408548\n",
            "Epoch 7600, Loss: 0.2499388641885592\n",
            "Epoch 7700, Loss: 0.24993764657224823\n",
            "Epoch 7800, Loss: 0.2499364086384499\n",
            "Epoch 7900, Loss: 0.2499351497110489\n",
            "Epoch 8000, Loss: 0.24993386909363968\n",
            "Epoch 8100, Loss: 0.24993256606861036\n",
            "Epoch 8200, Loss: 0.2499312398961872\n",
            "Epoch 8300, Loss: 0.24992988981343583\n",
            "Epoch 8400, Loss: 0.2499285150332175\n",
            "Epoch 8500, Loss: 0.24992711474309803\n",
            "Epoch 8600, Loss: 0.24992568810420596\n",
            "Epoch 8700, Loss: 0.24992423425003757\n",
            "Epoch 8800, Loss: 0.24992275228520544\n",
            "Epoch 8900, Loss: 0.24992124128412746\n",
            "Epoch 9000, Loss: 0.24991970028965232\n",
            "Epoch 9100, Loss: 0.24991812831161836\n",
            "Epoch 9200, Loss: 0.2499165243253417\n",
            "Epoch 9300, Loss: 0.2499148872700293\n",
            "Epoch 9400, Loss: 0.2499132160471125\n",
            "Epoch 9500, Loss: 0.24991150951849644\n",
            "Epoch 9600, Loss: 0.24990976650472044\n",
            "Epoch 9700, Loss: 0.2499079857830238\n",
            "Epoch 9800, Loss: 0.24990616608531113\n",
            "Epoch 9900, Loss: 0.24990430609601189\n",
            "Epoch 10000, Loss: 0.24990240444982598\n",
            "Epoch 10100, Loss: 0.2499004597293511\n",
            "Epoch 10200, Loss: 0.24989847046258162\n",
            "Epoch 10300, Loss: 0.24989643512027276\n",
            "Epoch 10400, Loss: 0.24989435211316147\n",
            "Epoch 10500, Loss: 0.24989221978903453\n",
            "Epoch 10600, Loss: 0.249890036429634\n",
            "Epoch 10700, Loss: 0.2498878002473908\n",
            "Epoch 10800, Loss: 0.24988550938197387\n",
            "Epoch 10900, Loss: 0.24988316189664356\n",
            "Epoch 11000, Loss: 0.24988075577439792\n",
            "Epoch 11100, Loss: 0.249878288913895\n",
            "Epoch 11200, Loss: 0.24987575912514004\n",
            "Epoch 11300, Loss: 0.2498731641249197\n",
            "Epoch 11400, Loss: 0.24987050153196605\n",
            "Epoch 11500, Loss: 0.24986776886183446\n",
            "Epoch 11600, Loss: 0.24986496352147292\n",
            "Epoch 11700, Loss: 0.24986208280346442\n",
            "Epoch 11800, Loss: 0.24985912387991704\n",
            "Epoch 11900, Loss: 0.2498560837959799\n",
            "Epoch 12000, Loss: 0.2498529594629571\n",
            "Epoch 12100, Loss: 0.24984974765099158\n",
            "Epoch 12200, Loss: 0.24984644498128847\n",
            "Epoch 12300, Loss: 0.24984304791784454\n",
            "Epoch 12400, Loss: 0.24983955275864872\n",
            "Epoch 12500, Loss: 0.2498359556263141\n",
            "Epoch 12600, Loss: 0.24983225245810087\n",
            "Epoch 12700, Loss: 0.24982843899528315\n",
            "Epoch 12800, Loss: 0.24982451077181328\n",
            "Epoch 12900, Loss: 0.24982046310222797\n",
            "Epoch 13000, Loss: 0.24981629106874104\n",
            "Epoch 13100, Loss: 0.2498119895074583\n",
            "Epoch 13200, Loss: 0.2498075529936486\n",
            "Epoch 13300, Loss: 0.24980297582599603\n",
            "Epoch 13400, Loss: 0.24979825200975442\n",
            "Epoch 13500, Loss: 0.24979337523871575\n",
            "Epoch 13600, Loss: 0.2497883388758992\n",
            "Epoch 13700, Loss: 0.24978313593285567\n",
            "Epoch 13800, Loss: 0.24977775904747684\n",
            "Epoch 13900, Loss: 0.24977220046018517\n",
            "Epoch 14000, Loss: 0.24976645198836983\n",
            "Epoch 14100, Loss: 0.24976050499892383\n",
            "Epoch 14200, Loss: 0.2497543503787191\n",
            "Epoch 14300, Loss: 0.2497479785028472\n",
            "Epoch 14400, Loss: 0.24974137920043027\n",
            "Epoch 14500, Loss: 0.249734541717794\n",
            "Epoch 14600, Loss: 0.249727454678769\n",
            "Epoch 14700, Loss: 0.2497201060418678\n",
            "Epoch 14800, Loss: 0.24971248305405816\n",
            "Epoch 14900, Loss: 0.24970457220082498\n",
            "Epoch 15000, Loss: 0.2496963591521829\n",
            "Epoch 15100, Loss: 0.24968782870426692\n",
            "Epoch 15200, Loss: 0.24967896471608952\n",
            "Epoch 15300, Loss: 0.24966975004101077\n",
            "Epoch 15400, Loss: 0.24966016645241895\n",
            "Epoch 15500, Loss: 0.24965019456306767\n",
            "Epoch 15600, Loss: 0.24963981373745397\n",
            "Epoch 15700, Loss: 0.249629001996556\n",
            "Epoch 15800, Loss: 0.2496177359141754\n",
            "Epoch 15900, Loss: 0.24960599050404264\n",
            "Epoch 16000, Loss: 0.24959373909675286\n",
            "Epoch 16100, Loss: 0.24958095320549106\n",
            "Epoch 16200, Loss: 0.24956760237938927\n",
            "Epoch 16300, Loss: 0.2495536540432201\n",
            "Epoch 16400, Loss: 0.24953907332198388\n",
            "Epoch 16500, Loss: 0.24952382284877153\n",
            "Epoch 16600, Loss: 0.24950786255409402\n",
            "Epoch 16700, Loss: 0.24949114943464754\n",
            "Epoch 16800, Loss: 0.24947363729923577\n",
            "Epoch 16900, Loss: 0.24945527648928523\n",
            "Epoch 17000, Loss: 0.2494360135710683\n",
            "Epoch 17100, Loss: 0.24941579099637978\n",
            "Epoch 17200, Loss: 0.24939454672799244\n",
            "Epoch 17300, Loss: 0.2493722138257345\n",
            "Epoch 17400, Loss: 0.24934871998848432\n",
            "Epoch 17500, Loss: 0.24932398704673941\n",
            "Epoch 17600, Loss: 0.24929793039969297\n",
            "Epoch 17700, Loss: 0.24927045838990927\n",
            "Epoch 17800, Loss: 0.24924147160772164\n",
            "Epoch 17900, Loss: 0.24921086211635401\n",
            "Epoch 18000, Loss: 0.2491785125874679\n",
            "Epoch 18100, Loss: 0.249144295335324\n",
            "Epoch 18200, Loss: 0.24910807123599013\n",
            "Epoch 18300, Loss: 0.24906968851596722\n",
            "Epoch 18400, Loss: 0.24902898139220567\n",
            "Epoch 18500, Loss: 0.24898576854265714\n",
            "Epoch 18600, Loss: 0.24893985138318953\n",
            "Epoch 18700, Loss: 0.2488910121227746\n",
            "Epoch 18800, Loss: 0.2488390115642251\n",
            "Epoch 18900, Loss: 0.24878358661225985\n",
            "Epoch 19000, Loss: 0.24872444744413472\n",
            "Epoch 19100, Loss: 0.24866127429026819\n",
            "Epoch 19200, Loss: 0.24859371376293984\n",
            "Epoch 19300, Loss: 0.2485213746599069\n",
            "Epoch 19400, Loss: 0.24844382315624547\n",
            "Epoch 19500, Loss: 0.24836057728134958\n",
            "Epoch 19600, Loss: 0.2482711005581559\n",
            "Epoch 19700, Loss: 0.24817479465747364\n",
            "Epoch 19800, Loss: 0.24807099089076393\n",
            "Epoch 19900, Loss: 0.24795894032851637\n",
            "Epoch 20000, Loss: 0.24783780228689684\n",
            "Epoch 20100, Loss: 0.24770663087053843\n",
            "Epoch 20200, Loss: 0.24756435919166692\n",
            "Epoch 20300, Loss: 0.247409780802015\n",
            "Epoch 20400, Loss: 0.24724152777026345\n",
            "Epoch 20500, Loss: 0.2470580447092659\n",
            "Epoch 20600, Loss: 0.24685755789832256\n",
            "Epoch 20700, Loss: 0.24663803844956106\n",
            "Epoch 20800, Loss: 0.24639715822654584\n",
            "Epoch 20900, Loss: 0.2461322369297424\n",
            "Epoch 21000, Loss: 0.24584017841033415\n",
            "Epoch 21100, Loss: 0.24551739385686366\n",
            "Epoch 21200, Loss: 0.24515970902047077\n",
            "Epoch 21300, Loss: 0.24476225211901567\n",
            "Epoch 21400, Loss: 0.2443193185254443\n",
            "Epoch 21500, Loss: 0.24382420787591574\n",
            "Epoch 21600, Loss: 0.2432690289605643\n",
            "Epoch 21700, Loss: 0.24264446789918875\n",
            "Epoch 21800, Loss: 0.24193951597791466\n",
            "Epoch 21900, Loss: 0.24114115557080604\n",
            "Epoch 22000, Loss: 0.24023400631720948\n",
            "Epoch 22100, Loss: 0.2391999396589004\n",
            "Epoch 22200, Loss: 0.23801767814359065\n",
            "Epoch 22300, Loss: 0.23666240600969662\n",
            "Epoch 22400, Loss: 0.23510542765141643\n",
            "Epoch 22500, Loss: 0.2333139172886963\n",
            "Epoch 22600, Loss: 0.2312508024697575\n",
            "Epoch 22700, Loss: 0.2288748137935358\n",
            "Epoch 22800, Loss: 0.22614071800601476\n",
            "Epoch 22900, Loss: 0.2229997473591639\n",
            "Epoch 23000, Loss: 0.21940027186097893\n",
            "Epoch 23100, Loss: 0.21528885939089634\n",
            "Epoch 23200, Loss: 0.2106120334638402\n",
            "Epoch 23300, Loss: 0.20531921770859118\n",
            "Epoch 23400, Loss: 0.19936742737335483\n",
            "Epoch 23500, Loss: 0.19272805654005398\n",
            "Epoch 23600, Loss: 0.1853954662774853\n",
            "Epoch 23700, Loss: 0.17739603115040292\n",
            "Epoch 23800, Loss: 0.16879521551172502\n",
            "Epoch 23900, Loss: 0.15969981475171607\n",
            "Epoch 24000, Loss: 0.15025332886454154\n",
            "Epoch 24100, Loss: 0.14062448237506445\n",
            "Epoch 24200, Loss: 0.13099123368837007\n",
            "Epoch 24300, Loss: 0.1215240068460069\n",
            "Epoch 24400, Loss: 0.11237171693216894\n",
            "Epoch 24500, Loss: 0.10365278291402251\n",
            "Epoch 24600, Loss: 0.0954515912642806\n",
            "Epoch 24700, Loss: 0.08781953851312263\n",
            "Epoch 24800, Loss: 0.08077914327893085\n",
            "Epoch 24900, Loss: 0.07432968914499866\n",
            "Epoch 25000, Loss: 0.06845318075854716\n",
            "Epoch 25100, Loss: 0.06311982388485995\n",
            "Epoch 25200, Loss: 0.05829262523448234\n",
            "Epoch 25300, Loss: 0.053930988748473095\n",
            "Epoch 25400, Loss: 0.0499933578878734\n",
            "Epoch 25500, Loss: 0.04643904037235504\n",
            "Epoch 25600, Loss: 0.04322938061870963\n",
            "Epoch 25700, Loss: 0.04032843997823443\n",
            "Epoch 25800, Loss: 0.03770332320639589\n",
            "Epoch 25900, Loss: 0.035324262474289374\n",
            "Epoch 26000, Loss: 0.03316454380624066\n",
            "Epoch 26100, Loss: 0.03120033797286778\n",
            "Epoch 26200, Loss: 0.02941047948525565\n",
            "Epoch 26300, Loss: 0.027776223278498498\n",
            "Epoch 26400, Loss: 0.02628099833279549\n",
            "Epoch 26500, Loss: 0.02491017011673525\n",
            "Epoch 26600, Loss: 0.023650818645741964\n",
            "Epoch 26700, Loss: 0.022491535530378175\n",
            "Epoch 26800, Loss: 0.021422241168815147\n",
            "Epoch 26900, Loss: 0.020434021854031148\n",
            "Epoch 27000, Loss: 0.019518985753497326\n",
            "Epoch 27100, Loss: 0.018670136286786078\n",
            "Epoch 27200, Loss: 0.0178812612410003\n",
            "Epoch 27300, Loss: 0.017146835933093\n",
            "Epoch 27400, Loss: 0.01646193878979189\n",
            "Epoch 27500, Loss: 0.015822177828675867\n",
            "Epoch 27600, Loss: 0.015223626661238934\n",
            "Epoch 27700, Loss: 0.01466276878365741\n",
            "Epoch 27800, Loss: 0.014136449063267012\n",
            "Epoch 27900, Loss: 0.013641831462648978\n",
            "Epoch 28000, Loss: 0.013176362165773826\n",
            "Epoch 28100, Loss: 0.012737737380711791\n",
            "Epoch 28200, Loss: 0.012323875190940841\n",
            "Epoch 28300, Loss: 0.011932890912863784\n",
            "Epoch 28400, Loss: 0.01156307549171416\n",
            "Epoch 28500, Loss: 0.0112128765326703\n",
            "Epoch 28600, Loss: 0.010880881619818112\n",
            "Epoch 28700, Loss: 0.01056580362367874\n",
            "Epoch 28800, Loss: 0.010266467739350261\n",
            "Epoch 28900, Loss: 0.009981800032800001\n",
            "Epoch 29000, Loss: 0.009710817303299705\n",
            "Epoch 29100, Loss: 0.009452618096120972\n",
            "Epoch 29200, Loss: 0.0092063747220244\n",
            "Epoch 29300, Loss: 0.008971326159315547\n",
            "Epoch 29400, Loss: 0.00874677173076276\n",
            "Epoch 29500, Loss: 0.00853206546187349\n",
            "Epoch 29600, Loss: 0.008326611039240608\n",
            "Epoch 29700, Loss: 0.008129857298190738\n",
            "Epoch 29800, Loss: 0.007941294178034958\n",
            "Epoch 29900, Loss: 0.007760449091050219\n",
            "Epoch 30000, Loss: 0.007586883658084984\n",
            "Epoch 30100, Loss: 0.007420190769536657\n",
            "Epoch 30200, Loss: 0.007259991935521556\n",
            "Epoch 30300, Loss: 0.007105934893459946\n",
            "Epoch 30400, Loss: 0.006957691445124243\n",
            "Epoch 30500, Loss: 0.006814955498526285\n",
            "Epoch 30600, Loss: 0.006677441292920897\n",
            "Epoch 30700, Loss: 0.006544881787733747\n",
            "Epoch 30800, Loss: 0.006417027198433586\n",
            "Epoch 30900, Loss: 0.0062936436643049685\n",
            "Epoch 31000, Loss: 0.006174512034773654\n",
            "Epoch 31100, Loss: 0.006059426762426639\n",
            "Epoch 31200, Loss: 0.005948194892176286\n",
            "Epoch 31300, Loss: 0.005840635137170508\n",
            "Epoch 31400, Loss: 0.005736577033065075\n",
            "Epoch 31500, Loss: 0.00563586016317031\n",
            "Epoch 31600, Loss: 0.005538333447775414\n",
            "Epoch 31700, Loss: 0.005443854491653841\n",
            "Epoch 31800, Loss: 0.005352288984373804\n",
            "Epoch 31900, Loss: 0.005263510148588156\n",
            "Epoch 32000, Loss: 0.005177398231966334\n",
            "Epoch 32100, Loss: 0.005093840038866701\n",
            "Epoch 32200, Loss: 0.005012728498233652\n",
            "Epoch 32300, Loss: 0.004933962264549681\n",
            "Epoch 32400, Loss: 0.004857445348980532\n",
            "Epoch 32500, Loss: 0.0047830867781263695\n",
            "Epoch 32600, Loss: 0.004710800278039086\n",
            "Epoch 32700, Loss: 0.004640503981385291\n",
            "Epoch 32800, Loss: 0.004572120155833593\n",
            "Epoch 32900, Loss: 0.00450557495192137\n",
            "Epoch 33000, Loss: 0.004440798168816086\n",
            "Epoch 33100, Loss: 0.0043777230365304835\n",
            "Epoch 33200, Loss: 0.004316286013278868\n",
            "Epoch 33300, Loss: 0.004256426596779591\n",
            "Epoch 33400, Loss: 0.004198087148413374\n",
            "Epoch 33500, Loss: 0.004141212729242172\n",
            "Epoch 33600, Loss: 0.004085750946979363\n",
            "Epoch 33700, Loss: 0.004031651813079611\n",
            "Epoch 33800, Loss: 0.00397886760918753\n",
            "Epoch 33900, Loss: 0.003927352762247769\n",
            "Epoch 34000, Loss: 0.0038770637276373055\n",
            "Epoch 34100, Loss: 0.0038279588797335787\n",
            "Epoch 34200, Loss: 0.0037799984093797026\n",
            "Epoch 34300, Loss: 0.003733144227751735\n",
            "Epoch 34400, Loss: 0.003687359876172794\n",
            "Epoch 34500, Loss: 0.0036426104414546538\n",
            "Epoch 34600, Loss: 0.0035988624763810383\n",
            "Epoch 34700, Loss: 0.0035560839249763896\n",
            "Epoch 34800, Loss: 0.0035142440522319424\n",
            "Epoch 34900, Loss: 0.003473313377985867\n",
            "Epoch 35000, Loss: 0.0034332636146772247\n",
            "Epoch 35100, Loss: 0.0033940676087151926\n",
            "Epoch 35200, Loss: 0.003355699285223546\n",
            "Epoch 35300, Loss: 0.003318133595938843\n",
            "Epoch 35400, Loss: 0.0032813464700568997\n",
            "Epoch 35500, Loss: 0.0032453147678367937\n",
            "Epoch 35600, Loss: 0.0032100162367860036\n",
            "Epoch 35700, Loss: 0.003175429470262584\n",
            "Epoch 35800, Loss: 0.0031415338683419567\n",
            "Epoch 35900, Loss: 0.003108309600806868\n",
            "Epoch 36000, Loss: 0.003075737572128874\n",
            "Epoch 36100, Loss: 0.003043799388318787\n",
            "Epoch 36200, Loss: 0.0030124773255320965\n",
            "Epoch 36300, Loss: 0.002981754300323175\n",
            "Epoch 36400, Loss: 0.0029516138414491075\n",
            "Epoch 36500, Loss: 0.0029220400631309526\n",
            "Epoch 36600, Loss: 0.002893017639686222\n",
            "Epoch 36700, Loss: 0.002864531781452041\n",
            "Epoch 36800, Loss: 0.0028365682119239626\n",
            "Epoch 36900, Loss: 0.002809113146040098\n",
            "Epoch 37000, Loss: 0.002782153269544928\n",
            "Epoch 37100, Loss: 0.002755675719371367\n",
            "Epoch 37200, Loss: 0.0027296680649834897\n",
            "Epoch 37300, Loss: 0.0027041182906260477\n",
            "Epoch 37400, Loss: 0.0026790147784304425\n",
            "Epoch 37500, Loss: 0.0026543462923295747\n",
            "Epoch 37600, Loss: 0.0026301019627374585\n",
            "Epoch 37700, Loss: 0.0026062712719517973\n",
            "Epoch 37800, Loss: 0.002582844040240373\n",
            "Epoch 37900, Loss: 0.002559810412574737\n",
            "Epoch 38000, Loss: 0.0025371608459764947\n",
            "Epoch 38100, Loss: 0.0025148860974438733\n",
            "Epoch 38200, Loss: 0.0024929772124279936\n",
            "Epoch 38300, Loss: 0.0024714255138301965\n",
            "Epoch 38400, Loss: 0.0024502225914934256\n",
            "Epoch 38500, Loss: 0.002429360292162042\n",
            "Epoch 38600, Loss: 0.0024088307098865417\n",
            "Epoch 38700, Loss: 0.0023886261768499752\n",
            "Epoch 38800, Loss: 0.002368739254595368\n",
            "Epoch 38900, Loss: 0.002349162725633672\n",
            "Epoch 39000, Loss: 0.0023298895854133666\n",
            "Epoch 39100, Loss: 0.0023109130346340148\n",
            "Epoch 39200, Loss: 0.0022922264718866314\n",
            "Epoch 39300, Loss: 0.0022738234866050196\n",
            "Epoch 39400, Loss: 0.0022556978523130076\n",
            "Epoch 39500, Loss: 0.0022378435201532605\n",
            "Epoch 39600, Loss: 0.0022202546126842753\n",
            "Epoch 39700, Loss: 0.0022029254179327267\n",
            "Epoch 39800, Loss: 0.002185850383689094\n",
            "Epoch 39900, Loss: 0.002169024112035146\n",
            "Epoch 40000, Loss: 0.002152441354092576\n",
            "Epoch 40100, Loss: 0.0021360970049822702\n",
            "Epoch 40200, Loss: 0.0021199860989847457\n",
            "Epoch 40300, Loss: 0.0021041038048924343\n",
            "Epoch 40400, Loss: 0.0020884454215450384\n",
            "Epoch 40500, Loss: 0.0020730063735397465\n",
            "Epoch 40600, Loss: 0.0020577822071083905\n",
            "Epoch 40700, Loss: 0.0020427685861541004\n",
            "Epoch 40800, Loss: 0.0020279612884403658\n",
            "Epoch 40900, Loss: 0.002013356201925722\n",
            "Epoch 41000, Loss: 0.001998949321237735\n",
            "Epoch 41100, Loss: 0.001984736744280161\n",
            "Epoch 41200, Loss: 0.0019707146689674974\n",
            "Epoch 41300, Loss: 0.001956879390081381\n",
            "Epoch 41400, Loss: 0.0019432272962437516\n",
            "Epoch 41500, Loss: 0.0019297548670015935\n",
            "Epoch 41600, Loss: 0.0019164586700186766\n",
            "Epoch 41700, Loss: 0.0019033353583697448\n",
            "Epoch 41800, Loss: 0.0018903816679327742\n",
            "Epoch 41900, Loss: 0.0018775944148753194\n",
            "Epoch 42000, Loss: 0.00186497049323099\n",
            "Epoch 42100, Loss: 0.0018525068725623553\n",
            "Epoch 42200, Loss: 0.0018402005957066814\n",
            "Epoch 42300, Loss: 0.0018280487766012253\n",
            "Epoch 42400, Loss: 0.0018160485981847307\n",
            "Epoch 42500, Loss: 0.0018041973103721342\n",
            "Epoch 42600, Loss: 0.0017924922280995615\n",
            "Epoch 42700, Loss: 0.001780930729436702\n",
            "Epoch 42800, Loss: 0.0017695102537640074\n",
            "Epoch 42900, Loss: 0.0017582283000120488\n",
            "Epoch 43000, Loss: 0.0017470824249606982\n",
            "Epoch 43100, Loss: 0.001736070241595704\n",
            "Epoch 43200, Loss: 0.0017251894175204204\n",
            "Epoch 43300, Loss: 0.0017144376734207292\n",
            "Epoch 43400, Loss: 0.0017038127815808182\n",
            "Epoch 43500, Loss: 0.0016933125644481367\n",
            "Epoch 43600, Loss: 0.0016829348932455369\n",
            "Epoch 43700, Loss: 0.0016726776866288133\n",
            "Epoch 43800, Loss: 0.0016625389093879086\n",
            "Epoch 43900, Loss: 0.0016525165711902498\n",
            "Epoch 44000, Loss: 0.001642608725364505\n",
            "Epoch 44100, Loss: 0.0016328134677234237\n",
            "Epoch 44200, Loss: 0.0016231289354240988\n",
            "Epoch 44300, Loss: 0.0016135533058645338\n",
            "Epoch 44400, Loss: 0.001604084795614931\n",
            "Epoch 44500, Loss: 0.0015947216593826907\n",
            "Epoch 44600, Loss: 0.0015854621890096345\n",
            "Epoch 44700, Loss: 0.001576304712500537\n",
            "Epoch 44800, Loss: 0.0015672475930816982\n",
            "Epoch 44900, Loss: 0.0015582892282884858\n",
            "Epoch 45000, Loss: 0.0015494280490809024\n",
            "Epoch 45100, Loss: 0.0015406625189861463\n",
            "Epoch 45200, Loss: 0.0015319911332671294\n",
            "Epoch 45300, Loss: 0.0015234124181162198\n",
            "Epoch 45400, Loss: 0.001514924929873192\n",
            "Epoch 45500, Loss: 0.0015065272542665804\n",
            "Epoch 45600, Loss: 0.0014982180056776337\n",
            "Epoch 45700, Loss: 0.001489995826426123\n",
            "Epoch 45800, Loss: 0.001481859386077181\n",
            "Epoch 45900, Loss: 0.0014738073807685952\n",
            "Epoch 46000, Loss: 0.0014658385325576387\n",
            "Epoch 46100, Loss: 0.0014579515887870118\n",
            "Epoch 46200, Loss: 0.0014501453214691665\n",
            "Epoch 46300, Loss: 0.0014424185266882937\n",
            "Epoch 46400, Loss: 0.0014347700240195376\n",
            "Epoch 46500, Loss: 0.0014271986559648503\n",
            "Epoch 46600, Loss: 0.0014197032874047769\n",
            "Epoch 46700, Loss: 0.0014122828050659084\n",
            "Epoch 46800, Loss: 0.0014049361170032355\n",
            "Epoch 46900, Loss: 0.001397662152097092\n",
            "Epoch 47000, Loss: 0.001390459859564135\n",
            "Epoch 47100, Loss: 0.0013833282084819445\n",
            "Epoch 47200, Loss: 0.0013762661873267651\n",
            "Epoch 47300, Loss: 0.001369272803524062\n",
            "Epoch 47400, Loss: 0.001362347083011307\n",
            "Epoch 47500, Loss: 0.0013554880698128356\n",
            "Epoch 47600, Loss: 0.0013486948256261652\n",
            "Epoch 47700, Loss: 0.0013419664294196222\n",
            "Epoch 47800, Loss: 0.001335301977040764\n",
            "Epoch 47900, Loss: 0.0013287005808353645\n",
            "Epoch 48000, Loss: 0.001322161369276568\n",
            "Epoch 48100, Loss: 0.0013156834866039084\n",
            "Epoch 48200, Loss: 0.0013092660924719699\n",
            "Epoch 48300, Loss: 0.0013029083616082466\n",
            "Epoch 48400, Loss: 0.0012966094834800663\n",
            "Epoch 48500, Loss: 0.0012903686619702202\n",
            "Epoch 48600, Loss: 0.001284185115061045\n",
            "Epoch 48700, Loss: 0.0012780580745267325\n",
            "Epoch 48800, Loss: 0.0012719867856335909\n",
            "Epoch 48900, Loss: 0.0012659705068480142\n",
            "Epoch 49000, Loss: 0.0012600085095519843\n",
            "Epoch 49100, Loss: 0.0012541000777658194\n",
            "Epoch 49200, Loss: 0.0012482445078779549\n",
            "Epoch 49300, Loss: 0.00124244110838164\n",
            "Epoch 49400, Loss: 0.0012366891996182447\n",
            "Epoch 49500, Loss: 0.00123098811352699\n",
            "Epoch 49600, Loss: 0.0012253371934010104\n",
            "Epoch 49700, Loss: 0.0012197357936494881\n",
            "Epoch 49800, Loss: 0.0012141832795656519\n",
            "Epoch 49900, Loss: 0.0012086790271005757\n",
            "Final Predictions:\n",
            "[[0.03313695]\n",
            " [0.96538442]\n",
            " [0.96660147]\n",
            " [0.03743169]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sigmoid activation function and its derivative\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Mean Squared Error Loss and its derivative\n",
        "def mse_loss(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "def mse_loss_derivative(y_true, y_pred):\n",
        "    return 2 * (y_pred - y_true) / y_true.size\n",
        "\n",
        "# Neural Network Class\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # Initialize weights and biases\n",
        "        self.weights_input_hidden = np.random.rand(input_size, hidden_size) - 0.5\n",
        "        self.bias_hidden = np.random.rand(hidden_size) - 0.5\n",
        "        self.weights_hidden_output = np.random.rand(hidden_size, output_size) - 0.5\n",
        "        self.bias_output = np.random.rand(output_size) - 0.5\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Compute hidden layer\n",
        "        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
        "        self.hidden_output = sigmoid(self.hidden_input)\n",
        "\n",
        "        # Compute output layer\n",
        "        self.output_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
        "        self.output = sigmoid(self.output_input)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, X, y, output, learning_rate):\n",
        "        # Output layer error and gradient\n",
        "        output_error = mse_loss_derivative(y, output)\n",
        "        output_gradient = output_error * sigmoid_derivative(output)\n",
        "\n",
        "        # Hidden layer error and gradient\n",
        "        hidden_error = np.dot(output_gradient, self.weights_hidden_output.T)\n",
        "        hidden_gradient = hidden_error * sigmoid_derivative(self.hidden_output)\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights_hidden_output -= learning_rate * np.dot(self.hidden_output.T, output_gradient)\n",
        "        self.bias_output -= learning_rate * np.sum(output_gradient, axis=0)\n",
        "        self.weights_input_hidden -= learning_rate * np.dot(X.T, hidden_gradient)\n",
        "        self.bias_hidden -= learning_rate * np.sum(hidden_gradient, axis=0)\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            output = self.forward(X)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = mse_loss(y, output)\n",
        "\n",
        "            # Backward pass\n",
        "            self.backward(X, y, output, learning_rate)\n",
        "\n",
        "            # Print loss every 100 epochs\n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Example Dataset: XOR Problem\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = 2\n",
        "hidden_size = 4\n",
        "output_size = 1\n",
        "epochs = 50000\n",
        "learning_rate = 0.2\n",
        "\n",
        "# Create and train the neural network\n",
        "nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
        "nn.train(X, y, epochs, learning_rate)\n",
        "\n",
        "# Test the neural network\n",
        "print(\"Final Predictions:\")\n",
        "print(nn.forward(X))\n"
      ]
    }
  ]
}